---
title: "Modelamiento del Riesgo de Cr√©dito mediante Redes Neuronales Artificiales"
author: 
  - "Juan Jos√© Correa Hurtado"
  - "Jacobo Ochoa Ram√≠rez"
institute: "Universidad Nacional de Colombia"
course: "Redes Neuronales y Algoritmos Bioinspirados"
professor: "Juan David Ospina Arango"
date: "`13/05/2025"
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
execute:
  echo: false
   
---
![](images/logo_UNAL.png){width=200px}


<!-- ```{=latex}
\listoftables

``` -->

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import confusion_matrix

def plot_confusion_matrices(y_true, y_pred, labels=[0, 1], title="Matriz de Confusi√≥n"):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Porcentaje por fila

    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # Matriz absoluta
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels, ax=axes[0])
    axes[0].set_title('Conteos absolutos')
    axes[0].set_xlabel('Etiqueta predicha')
    axes[0].set_ylabel('Etiqueta real')

    # Matriz porcentual
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues',
                xticklabels=labels, yticklabels=labels, ax=axes[1])
    axes[1].set_title('Porcentajes (%)')
    axes[1].set_xlabel('Etiqueta predicha')
    axes[1].set_ylabel('Etiqueta real')

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()


```

# Introducci√≥n
El acceso al cr√©dito ha sido una herramienta clave en la inclusi√≥n financiera, pero tambi√©n representa un riesgo significativo para las entidades prestamistas. Anticipar el comportamiento de pago de los solicitantes de cr√©dito permite a las organizaciones mitigar p√©rdidas y ofrecer condiciones m√°s adecuadas a sus clientes.

Este proyecto tiene como objetivo desarrollar un modelo predictivo que estime la probabilidad de que una persona incurra en incumplimiento de pago en un cr√©dito otorgado. Para ello, se utilizar√° una red neuronal artificial cuya arquitectura ser√° optimizada con base en m√©tricas de desempe√±o y t√©cnicas de validaci√≥n cruzada. Adem√°s, se construir√° una scorecard para representar los resultados de manera interpretable.

Se debe hacer claridad que el modelo pretende determinar si un cliente es malo, codificando la variable *loan_status*, anulando registros NA e identificando registros de clientes *malos* como 1 y aquellos que no como 0.

El producto final de este proyecto se encuentra desplegado en la p√°gina:

# Metodolog√≠a

## Objetivos del proyecto
Con el desarrollo del proyecto se debe cumplir con los siguientes objetivos:

* El modelo clasifica satisfactoriamente a un cliente como *malo* o *no malo*.
* El modelo es relevante y pr√°ctico en el contexto econ√≥mico colombiano.
* El modelo utiliza informaci√≥n relevante y de f√°cil acceso para el usuario.
* Identificar las variables m√°s relevantes en el comportamiento crediticio.

## Limitaciones
Para el desarrollo de este proyecto se van determinar unas limitaciones en cuanto a lo que se va a realizar para el procesamiento de los datos y dise√±o del modelo, estas son:

* No se van a realizar procesos de procesamiento de lenguaje natural, ya que esto va m√°s all√° de los conocimientos compartidos en el curso hasta este punto. Esto implica que variables como *desc* no seran consideradas para la construcci√≥n del modelo.
* No se van a tomar en cuenta variables con informaci√≥n geogr√°fica. A pesar de que estas pueden aportar informaci√≥n valiosa es de crucial importancia mantener la utilidad y pr√°cticidad del modelo. Dado que el dataset utilizado fue recopilado en EE. UU. esta informaci√≥n ser√≠a irrelevante para el contexto econ√≥mico de Colombia.
* Solo se van a considerar cr√©ditos individuales, por lo que los registros correspondientes a creditos conjuntos ser√°n descartados.
* No se van a utilzar variables que indiquen fechas ya que no se conoce el tiempo desde que fueron recopilados los datos. Esta informaci√≥n no tendr√≠a ningun valor para la construcci√≥n del modelo.

## Fases de desarrollo

Se definen las siguientes fases de desarrollo del modelo:

1. Definici√≥n de metodolog√≠a del proyecto.
2. An√°lisis exploratorio de los datos.
3. Definici√≥n de variable objetivo e hip√≥tesis.
4. Construcci√≥n y optimizaci√≥n del modelo.
5. Construcci√≥n de √°rbol de decisi√≥n como modelo de baja complejidad de referencia.
6. Creaci√≥n de scorecard.
7. Listado de aprendizajes.
8. Planteamiento de caso de uso del modelo.
8. Redacci√≥n de reporte t√©cnico.
9. Desarrollo de aplicaci√≥n web.
10. Creaci√≥n de video publicitario.

Es importante mencionar que no todas las fases de desarrollo ser√°n llevadas a cabo de manera sequencial o en orden como el desarrollo de la aplicaci√≥n web que ser√° paralelo al desarrollo del modelo.


# An√°lisis Exploratorio de los Datos (EAD)

Para visualizar en detalle el EAD y la construcci√≥n del modelo v√©ase el siguiete notebook en el que se ejecut√≥ el proyecto: 
https://colab.research.google.com/drive/1U-OEjypK1s65GYBDMmmGsJf2vHo88CTr#scrollTo=dbHz6z6JNrR3

Se realiz√≥ el siguiente procedimiento:

1. Eliminaci√≥n de variables acorde a limitaciones del proyecto

    - Eliminaci√≥n de variables irrelevantes para el modelo
    - Eliminaci√≥n de registros "JOINT" y columnas asociadas
    - Eliminaci√≥n de variables que requieren de PLN para ser de utilidad
    - Eliminaci√≥n de variables con informaci√≥n geogr√°fica
    - Eliminaci√≥n de variables con fechas

2. Se identificaron y eliminaron variables post-resultado, las cuales causaban **data-leakage**

    Se identificaron como todas aquellas que tengan las palabras "rec", "pymnt" y "out" en el nombre, a excepci√≥n de la variable "mths_since_last_record".

3. Manejo de valores nulos


    Se identificaron 11 variables cuya cantidad de valores nulos supera el 5%. Para la mayor√≠a de estas variables  valor se identific√≥ que un valor nulo no necesariamente representa un registro faltante. Por este motivo se opta por codificar los valores nulos como su significado en el contexto de la variable. Aquellas cuyos valores nulos no se les encontr√≥ significancia fueron descartadas.

    Adicionalmente se gener√≥ una variable bandera por cada una de las variables anteriormente codificadas para analizar si la existencia de los valores nulos tiene alg√∫n valor predictivo.

    Se imputa con la mediana los registros cuyo indice de valores NA sean menores que el 5%. 


4. Se codific√≥ la variable objetivo de acuerdo a las siguientes indicaciones:



| Estado del Cr√©dito                                                       | Codificaci√≥n | Justificaci√≥n                                                  |
|--------------------------------------------------------------------------|--------------|----------------------------------------------------------------|
| Current                                                                  | NA           | No hay certeza sobre su comportamiento final de pago.         |
| Fully Paid                                                               | 0            | Son buenos pagadores confirmados.                              |
| Charged Off                                                              | 1            | Son definitivamente malos pagadores.                           |
| Late (31-120 days)                                                       | 1            | Se considerar√°n como malos pagadores.                          |
| Issued                                                                   | NA           | No hay informaci√≥n sobre comportamiento de pago.              |
| In Grace Period                                                          | NA           | A√∫n no est√°n obligados a pagar.                                |
| Late (16-30 days)                                                        | NA           | No hay certeza sobre su comportamiento final.                 |
| Does not meet the credit policy. Status:Fully Paid                      | 0            | Son buenos pagadores confirmados.                              |
| Default                                                                  | 1            | Son definitivamente malos pagadores.                           |
| Does not meet the credit policy. Status:Charged Off                     | 1            | Son definitivamente malos pagadores.                           |

  
5. Se realiza an√°lisis de correlaci√≥n y se opta por conservar una √∫nica variable bandera ya que estaban altamente correlacionadas entre s√≠.

![](images/correlacion.png){width=800px}

6. Se realiza la codificaci√≥n tipo *One-hot-encoding* con valores de -1/1 para las variables categ√≥ricas.

7. Se normalizan las variables num√©ricas en el intervalo [-1,1]. Adicionalmente se conservan los valores m√°ximos y m√≠nimos en archivos .csv para ser utilizados en la normalizaci√≥n del modelo de producci√≥n.



# Construcci√≥n y Optimizaci√≥n del Modelo

## Construcci√≥n

**Clase NN (Red Neuronal)**
    La clase NN representa la arquitectura del modelo de red neuronal utilizado para la clasificaci√≥n binaria. Fue implementada mediante PyTorch (nn.Module) y su prop√≥sito es modelar relaciones no lineales entre las variables predictoras y la probabilidad de incumplimiento de un pr√©stamo.

- Estructura del modelo:

    * Capa 1: Linear(input_size, 512) ‚Äì capa totalmente conectada con funci√≥n de activaci√≥n ReLU.

    * Dropout: Se aplica una capa de regularizaci√≥n con probabilidad configurable (ej. 0.3) para prevenir sobreajuste.

    * Capa de salida: Linear(512, 1) ‚Äì salida lineal que se conecta a una funci√≥n de p√©rdida sigmoide impl√≠cita (via BCEWithLogitsLoss).

    * M√©todo forward: Define el flujo de datos a trav√©s de la red, aplicando la activaci√≥n y el dropout entre capas.

Esta clase es flexible y permite ajustar la arquitectura f√°cilmente modificando el n√∫mero de neuronas o capas.

**Clase Trainer**
    La clase Trainer encapsula todo el proceso de entrenamiento, validaci√≥n y evaluaci√≥n del modelo. Fue dise√±ada para separar la l√≥gica del entrenamiento del resto del flujo del proyecto, facilitando la reproducibilidad y modularidad.

* Constructor (__init__):

    - Recibe el modelo, los datos ya particionados (entrenamiento, validaci√≥n, prueba), pesos de clase y rutas para guardar artefactos.

    - Aplica estandarizaci√≥n (con StandardScaler) si se utiliza entrenamiento completo.

* M√©todo create_dataloaders:

    - Convierte los conjuntos de datos en objetos DataLoader para entrenamiento por lotes.

* M√©todo train:

    - Ejecuta el entrenamiento en m√∫ltiples √©pocas.

    - Utiliza la funci√≥n de p√©rdida BCEWithLogitsLoss con ponderaci√≥n de clases para manejar el desbalance.

    - Emplea ReduceLROnPlateau para ajustar la tasa de aprendizaje autom√°ticamente.

    - Implementa early stopping basado en la p√©rdida de validaci√≥n.

    - Guarda autom√°ticamente el mejor modelo en disco (en la carpeta Models/).

* M√©todo evaluate:

    - Calcula predicciones en el conjunto de prueba y reporta m√©tricas como accuracy, F1, recall, y balanced accuracy.

    - Guarda los resultados en archivos .json para an√°lisis posterior.


## Optimizaci√≥n

Para tener un punto de referencia se entren√≥ un modelo con todas las variables disponibles para luego compararse con un n√∫mero de variables razonable para un usuario que est√° llenando un formulario y tiene tiempo e informaci√≥n limitada.

```{python}

```
### Modelo Completo
```{python}
import json
import os

# Cargar m√©tricas (ajusta la ruta si es necesario)
ruta_metricas = os.path.join("utils", "Metrics", "modelo_completov4.json")
with open(ruta_metricas, "r") as f:
    metricas = json.load(f)

# Simular predicciones reales si no tienes y_true/y_pred
# Aqu√≠ los reconstruimos desde los valores de la matriz
y_true = [0] * (metricas["true_negatives"] + metricas["false_positives"]) + [1] * (metricas["false_negatives"] + metricas["true_positives"])
y_pred = ([0] * metricas["true_negatives"] + [1] * metricas["false_positives"] +
          [0] * metricas["false_negatives"] + [1] * metricas["true_positives"])

# Llamar funci√≥n
plot_confusion_matrices(y_true, y_pred)
```

### Modelo Reducido 
Para la definici√≥n de este modelo se utilizaron las siguientes variables


| Variable                               | Descripci√≥n                                                      |
|----------------------------------------|------------------------------------------------------------------|
| `int_rate`                             | Tasa de inter√©s del pr√©stamo                                     |
| `term_ 60 months`                      | Duraci√≥n del pr√©stamo en meses (60 meses)                        |
| `dti`                                  | Relaci√≥n deuda-ingreso del solicitante                           |
| `verification_status_Source Verified` | Verificaci√≥n del ingreso por la fuente original                  |
| `revol_util`                           | Utilizaci√≥n del cr√©dito renovable (%)                            |
| `installment`                          | Cuota mensual del pr√©stamo                                       |
| `home_ownership`                       | Tipo de tenencia de la vivienda (alquilada, propia, hipotecada)  |
| `sub_grade`                            | Subcategor√≠a de riesgo crediticio asignada al pr√©stamo           |
| `purpose`                              | Prop√≥sito declarado del pr√©stamo                                 |

```{python}

ruta_metricas = os.path.join("utils", "Metrics", "10_mas_importantes_y_dummiesv7.json")
with open(ruta_metricas, "r") as f:
    metricas = json.load(f)

# Simular predicciones reales si no tienes y_true/y_pred
# Aqu√≠ los reconstruimos desde los valores de la matriz
y_true = [0] * (metricas["true_negatives"] + metricas["false_positives"]) + [1] * (metricas["false_negatives"] + metricas["true_positives"])
y_pred = ([0] * metricas["true_negatives"] + [1] * metricas["false_positives"] +
          [0] * metricas["false_negatives"] + [1] * metricas["true_positives"])

# Llamar funci√≥n
plot_confusion_matrices(y_true, y_pred)
```


### Modelo Reducido para Recall
Con estas mismas variables se entren√≥ un segundo modelo cuyo objetivo es m√°ximizar la metrica recall para la clase 1. Esto se logr√≥ modificando el peso que recibe la funci√≥n `nn.BCEWithLogitsLoss`.

Este cambio se realizo con el objetivo de minimizar el n√∫mero de clientes *"malos"* que puedan ser categorizados como clientes *"no malos"*. Esto es crucial para instituciones prestadoras, puesto que realizar prestamos a clientes con un p√©rfil de riesgo peligroso puede representar una perdida asegurada.


```{python}

ruta_metricas = os.path.join("utils", "Metrics", "10_mas_importantes_y_dummiesvBiggerRecall.json")
with open(ruta_metricas, "r") as f:
    metricas = json.load(f)

# Simular predicciones reales si no tienes y_true/y_pred
# Aqu√≠ los reconstruimos desde los valores de la matriz
y_true = [0] * (metricas["true_negatives"] + metricas["false_positives"]) + [1] * (metricas["false_negatives"] + metricas["true_positives"])
y_pred = ([0] * metricas["true_negatives"] + [1] * metricas["false_positives"] +
          [0] * metricas["false_negatives"] + [1] * metricas["true_positives"])

# Llamar funci√≥n
plot_confusion_matrices(y_true, y_pred)
```

Este es el modelo que se va a utilizar en el despliegue de la p√°gina.

##### Importancia de las Variables

Se utiliz√≥ la t√©cnica `Feature Importance via Permutation` para analizar la incidencia de cada una de las variables en el resultado final del modelo

![](images/feature_importance.png){width=800px}

Para la m√©trica objetivo *recall* se obtiene que la caracteristica m√°s importante es el subgrado A2 y es bastante claro que la variabe `subgrade` tuvo un gran efecto en el entrenamiento del modelo.

# Modelo de Baja Complejidad

Se opt√≥ por construir un arbol binario como modelo de baja complejidad porque este es de f√°cil interpretaci√≥n y se puede analizar con una representaci√≥n gr√°fica. Esto puede ayudar a entender mejor los conceptos, muy diferente a un red neuronal que muchas veces es denomminada como *caja negra*.

La ra√≠z del arbol construido tiene la siguiente estructura:

![](images/arbol_binario.jpg){width=800px}

## Interpretaci√≥n de √Årboles de Decisi√≥n y Comparaci√≥n con Redes Neuronales

El √°rbol de decisi√≥n divide el conjunto de datos en ramas, tomando decisiones binarias en cada nodo interno seg√∫n una variable.  
En nuestro caso, una de esas variables clave es la tasa de inter√©s (`int_rate`).  
El √°rbol decide qu√© variable usar para dividir en cada nodo con base en una **medida de impureza**, la cual eval√∫a qu√© tan puras son las divisiones.  
Un nodo es considerado puro cuando contiene observaciones de una sola clase.

Este tipo de modelo permite detectar relaciones **no lineales** y combinaciones de condiciones.  
Por ejemplo:

> Las personas con tasa de inter√©s alta (`int_rate`) y bajo ingreso (`annual_inc`) podr√≠an tener mayor riesgo de incumplimiento.

Estas condiciones no se interpretan de forma aislada, sino **conjuntamente**.

A continuaci√≥n, se presenta una comparaci√≥n entre √°rboles de decisi√≥n y redes neuronales:

| **Aspecto**                   | **√Årbol de decisi√≥n** | **Red neuronal**           |
|------------------------------|------------------------|-----------------------------|
| Interpretabilidad            | Alta                   | Baja                        |
| Complejidad de patrones      | Baja-Media             | Alta                        |
| Requiere escalado            | No                     | S√≠                          |
| Datos desbalanceados         | Afectan mucho          | Se puede mitigar con t√©cnicas adicionales |
| Tiempo de entrenamiento      | R√°pido                 | Puede ser m√°s lento         |



# Creaci√≥n de Scorecard

Dado que ya se ha definido el modelo que ser√° utilizado en el despliegue, se construye la siguiente *ScoreCard*:

![](images/scorecard.png){width=800px}


# Listado de Aprendizajes

## Aprendizajes del Proyecto

**Importancia de la limpieza de datos**  
El proceso de depuraci√≥n de variables y registros irrelevantes fue esencial para garantizar que el modelo se entrene sobre informaci√≥n √∫til y no contaminada, evitando fen√≥menos como el *data leakage*.

**Balance entre desempe√±o y simplicidad**  
Se comprob√≥ que es posible construir modelos con un n√∫mero reducido de variables que mantengan un desempe√±o competitivo. Esto es crucial para facilitar la implementaci√≥n pr√°ctica en escenarios donde el *input* del usuario es limitado.

**Impacto del desbalance de clases**  
La necesidad de ajustar los pesos en la funci√≥n de p√©rdida para mejorar m√©tricas como el *recall* evidencia c√≥mo un desbalance puede afectar fuertemente los resultados del modelo y la toma de decisiones.

**El valor del an√°lisis exploratorio profundo**  
Un an√°lisis exploratorio riguroso permiti√≥ tomar decisiones informadas sobre imputaci√≥n de valores nulos, codificaci√≥n de variables y selecci√≥n de caracter√≠sticas.

**Ventajas de modularizar el c√≥digo**  
Dise√±ar clases como `NN` y `Trainer` permiti√≥ mantener el c√≥digo organizado, reutilizable y f√°cilmente ajustable durante las fases de prueba y optimizaci√≥n.

**Importancia de m√©tricas m√∫ltiples**  
M√°s all√° del *accuracy*, m√©tricas como *f1-score*, *recall* y *balanced accuracy* permiten una evaluaci√≥n m√°s precisa del modelo, especialmente en contextos con clases desbalanceadas y contextos especificos como lo son las decisiones para entidades bancarias.

**Limitaciones contextuales del dataset**  
Se reconoci√≥ que ciertos datos, como los geogr√°ficos o financieros espec√≠ficos de EE. UU., no son directamente aplicables al contexto colombiano y deben ser descartados para mantener la relevancia.

**Interpretabilidad mediante scorecard**  
La construcci√≥n de una *scorecard* permiti√≥ transformar las salidas del modelo en puntajes interpretables para usuarios no t√©cnicos. Esta herramienta facilita la comunicaci√≥n del nivel de riesgo asociado a un solicitante de cr√©dito. Adem√°s, la distribuci√≥n de los puntajes permiti√≥ observar que la mayor√≠a de los individuos se agrupan en el rango medio de riesgo, lo cual puede ser √∫til para definir umbrales de decisi√≥n personalizados.



# Bibliograf√≠a

1. Faressayah. (2023, 31 enero).  Lending Club Loan üí∞ Defaulters üèÉ ‚ôÇ prediction. Kaggle. https://www.kaggle.com/code/faressayah/lending-club-loan-defaulters-prediction#%E2%9C%94%EF%B8%8F-loan_status


2. Mahmoudalisalem. (2024, 24 octubre). Credit Risk Prediction | ANN96 vs XGBoost95. Kaggle. https://www.kaggle.com/code/mahmoudalisalem/credit-risk-prediction-ann96-vs-xgboost95


3. Acharya, M. (2025, 21 abril). Letters of Credit ‚Äì Definition, types & process. Cleartax. https://cleartax.in/s/letters-of-credit


4. Mahoney, A. J. (2025, 24 enero). Credit Risk Modeling with Machine Learning. Towards Data Science. https://towardsdatascience.com/credit-risk-modeling-with-machine-learning-8c8a2657b4c4/




